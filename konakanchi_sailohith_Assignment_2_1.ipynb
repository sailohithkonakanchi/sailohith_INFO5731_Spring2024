{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a5c8ba-4e1d-4802-a71e-3eb950a15a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 25 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 50 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 75 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 100 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 125 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 150 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 175 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 200 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 225 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 250 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 275 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 300 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 325 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 350 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 375 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 400 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 425 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 450 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 475 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 500 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 525 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 550 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 575 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 600 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 625 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 650 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 675 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 700 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 725 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 750 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 775 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 800 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 825 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 850 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 875 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 900 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 925 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 950 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 975 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Collected 1000 reviews for https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2\n",
            "Successfully saved reviews to movie_reviews_with_ratings.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(movie_urls):\n",
        "    all_reviews_with_ratings = []\n",
        "    for movie_url, release_date, stars in movie_urls:\n",
        "        reviews_with_ratings = []\n",
        "        page = 1\n",
        "        while len(reviews_with_ratings) < 1000:\n",
        "            response = requests.get(movie_url + f'&start={page * 10}')\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                review_containers = soup.find_all('div', class_='review-container')\n",
        "                if not review_containers:\n",
        "                    break\n",
        "                for container in review_containers:\n",
        "                    review_text = container.find('div', class_='text show-more__control').text.strip()\n",
        "                    rating_element = container.find('span', class_='rating-other-user-rating')\n",
        "                    if rating_element:\n",
        "                        rating = rating_element.find('span').text.strip()\n",
        "                    else:\n",
        "                        rating = 'Not Rated'\n",
        "                    reviews_with_ratings.append({'Review': review_text, 'Rating': rating})\n",
        "                    if len(reviews_with_ratings) >= 1000:\n",
        "                        break\n",
        "                print(f\"Collected {len(reviews_with_ratings)} reviews for {movie_url}\")\n",
        "                page += 1\n",
        "            else:\n",
        "                print(f\"Failed to retrieve data from {movie_url}\")\n",
        "                break\n",
        "        all_reviews_with_ratings.extend(reviews_with_ratings)\n",
        "    return all_reviews_with_ratings\n",
        "\n",
        "def save_reviews_to_csv(reviews, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Review'])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review['Review']])\n",
        "\n",
        "\n",
        "def main():\n",
        "    movie_data = [\n",
        "        ('https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ql_2', '2023-01-01', 8.5),\n",
        "        # Add URLs, release dates, and stars for more movies if needed\n",
        "    ]\n",
        "    reviews_with_ratings = scrape_imdb_reviews(movie_data)\n",
        "    if len(reviews_with_ratings) >= 1000:\n",
        "        save_reviews_to_csv(reviews_with_ratings[:1000], 'movie_reviews_with_ratings.csv')\n",
        "        print(\"Successfully saved reviews to movie_reviews_with_ratings.csv\")\n",
        "    else:\n",
        "        print(\"Failed to collect enough reviews.\")\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25d050a-d733-45db-b4cd-e7ec65f62c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "                                              Review\n",
            "0  Margot does the best with what she's given, bu...\n",
            "1  Before making Barbie (2023), Greta Gerwig sing...\n",
            "2  The quality, the humor, and the writing of the...\n",
            "3  As much as it pains me to give a movie called ...\n",
            "4  As a woman that grew up with Barbie, I was ver...\n",
            "Step 1: Remove noise, special characters, and punctuations:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  Margot does the best with what shes given but ...  \n",
            "1  Before making Barbie  Greta Gerwig singlehande...  \n",
            "2  The quality the humor and the writing of the m...  \n",
            "3  As much as it pains me to give a movie called ...  \n",
            "4  As a woman that grew up with Barbie I was very...  \n",
            "Step 2: Remove numbers:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  Margot does the best with what shes given but ...  \n",
            "1  Before making Barbie  Greta Gerwig singlehande...  \n",
            "2  The quality the humor and the writing of the m...  \n",
            "3  As much as it pains me to give a movie called ...  \n",
            "4  As a woman that grew up with Barbie I was very...  \n",
            "Step 3: Remove stopwords:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  Margot best shes given film disappointing mark...  \n",
            "1  making Barbie Greta Gerwig singlehandedly dire...  \n",
            "2  quality humor writing movie fun quirky unique ...  \n",
            "3  much pains give movie called Barbie brilliantl...  \n",
            "4  woman grew Barbie excited movie curious see wo...  \n",
            "Step 4: Lowercase all texts:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  margot best shes given film disappointing mark...  \n",
            "1  making barbie greta gerwig singlehandedly dire...  \n",
            "2  quality humor writing movie fun quirky unique ...  \n",
            "3  much pains give movie called barbie brilliantl...  \n",
            "4  woman grew barbie excited movie curious see wo...  \n",
            "Step 5: Stemming:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  margot best she given film disappoint market f...  \n",
            "1  make barbi greta gerwig singlehandedli direct ...  \n",
            "2  qualiti humor write movi fun quirki uniqu get ...  \n",
            "3  much pain give movi call barbi brilliantli han...  \n",
            "4  woman grew barbi excit movi curiou see would e...  \n",
            "Step 6: Lemmatization:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  margot best she given film disappoint market f...  \n",
            "1  make barbi greta gerwig singlehandedli direct ...  \n",
            "2  qualiti humor write movi fun quirki uniqu get ...  \n",
            "3  much pain give movi call barbi brilliantli han...  \n",
            "4  woman grew barbi excit movi curiou see would e...  \n",
            "\n",
            "Cleaned Data:\n",
            "                                              Review  \\\n",
            "0  Margot does the best with what she's given, bu...   \n",
            "1  Before making Barbie (2023), Greta Gerwig sing...   \n",
            "2  The quality, the humor, and the writing of the...   \n",
            "3  As much as it pains me to give a movie called ...   \n",
            "4  As a woman that grew up with Barbie, I was ver...   \n",
            "\n",
            "                                        clean_review  \n",
            "0  margot best she given film disappoint market f...  \n",
            "1  make barbi greta gerwig singlehandedli direct ...  \n",
            "2  qualiti humor write movi fun quirki uniqu get ...  \n",
            "3  much pain give movi call barbi brilliantli han...  \n",
            "4  woman grew barbi excit movi curiou see would e...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data from CSV\n",
        "df = pd.read_csv('/content/movie_reviews_with_ratings.csv')\n",
        "print(\"Original Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Remove noise, special characters, and punctuations\n",
        "df['clean_review'] = df['Review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "print(\"Step 1: Remove noise, special characters, and punctuations:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "# Step 2: Remove numbers\n",
        "df['clean_review'] = df['clean_review'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "print(\"Step 2: Remove numbers:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "# Step 3: Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['clean_review'] = df['clean_review'].apply(lambda x: ' '.join(word for word in x.split() if word.lower() not in stop_words))\n",
        "print(\"Step 3: Remove stopwords:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "# Step 4: Lowercase all texts\n",
        "df['clean_review'] = df['clean_review'].apply(lambda x: x.lower())\n",
        "print(\"Step 4: Lowercase all texts:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "# Step 5: Stemming\n",
        "porter = PorterStemmer()\n",
        "df['clean_review'] = df['clean_review'].apply(lambda x: ' '.join(porter.stem(word) for word in x.split()))\n",
        "print(\"Step 5: Stemming:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "# Step 6: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['clean_review'] = df['clean_review'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
        "print(\"Step 6: Lemmatization:\")\n",
        "print(df[['Review', 'clean_review']].head())\n",
        "\n",
        "df.to_csv('/content/movie_reviews_with_ratings.csv', index=False)\n",
        "print(\"\\nCleaned Data:\")\n",
        "print(df[['Review', 'clean_review']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a0f531-eb5d-4691-950d-a32bde1371fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Counts:\n",
            "NOUN: 27\n",
            "VERB: 13\n",
            "ADJ: 9\n",
            "ADV: 4\n",
            "\n",
            "Dependency Parsing Trees:\n",
            "margot --> advmod --> best\n",
            "best --> advmod --> given\n",
            "she --> nsubj --> given\n",
            "given --> ROOT --> given\n",
            "film --> compound --> satir\n",
            "disappoint --> compound --> market\n",
            "market --> compound --> fun\n",
            "fun --> compound --> satir\n",
            "quirki --> compound --> satir\n",
            "satir --> compound --> movi\n",
            "homag --> compound --> movi\n",
            "movi --> nsubj --> start\n",
            "start --> dobj --> given\n",
            "way --> advmod --> start\n",
            "end --> conj --> given\n",
            "overdramat --> compound --> speech\n",
            "speech --> nsubj --> end\n",
            "end --> nsubj --> make\n",
            "clearli --> compound --> tri\n",
            "tri --> nsubj --> make\n",
            "make --> ccomp --> end\n",
            "audienc --> nsubj --> feel\n",
            "feel --> ccomp --> make\n",
            "someth --> advmod --> left\n",
            "left --> amod --> everyon\n",
            "everyon --> nsubj --> feel\n",
            "feel --> ccomp --> feel\n",
            "confus --> nsubj --> say\n",
            "say --> ccomp --> feel\n",
            "i --> nsubj --> m\n",
            "m --> ccomp --> say\n",
            "crotcheti --> npadvmod --> old\n",
            "old --> amod --> man\n",
            "man --> dobj --> given\n",
            "i --> nsubj --> m\n",
            "m --> ROOT --> m\n",
            "woman --> attr --> m\n",
            "i --> nsubj --> m\n",
            "m --> ROOT --> m\n",
            "pretti --> npadvmod --> sure\n",
            "sure --> ROOT --> sure\n",
            "i --> nsubj --> m\n",
            "m --> aux --> victim\n",
            "movi --> nmod --> target\n",
            "target --> compound --> parent\n",
            "audienc --> npadvmod --> saddest\n",
            "saddest --> amod --> parent\n",
            "part --> compound --> parent\n",
            "parent --> compound --> theater\n",
            "kid --> compound --> theater\n",
            "theater --> compound --> victim\n",
            "victim --> nmod --> movi\n",
            "poor --> amod --> kid\n",
            "market --> compound --> kid\n",
            "kid --> compound --> movi\n",
            "movi --> nsubj --> look\n",
            "overal --> amod --> humor\n",
            "humor --> compound --> beauti\n",
            "fun --> compound --> occas\n",
            "occas --> compound --> film\n",
            "film --> compound --> beauti\n",
            "beauti --> nsubj --> look\n",
            "look --> ccomp --> sure\n",
            "whole --> amod --> concept\n",
            "concept --> nsubj --> fall\n",
            "fall --> ccomp --> sure\n",
            "apart --> advmod --> fall\n",
            "second --> amod --> half\n",
            "half --> amod --> film\n",
            "film --> nmod --> woman\n",
            "becom --> compound --> parti\n",
            "piti --> compound --> parti\n",
            "parti --> appos --> film\n",
            "strong --> amod --> woman\n",
            "woman --> npadvmod --> sure\n",
            "\n",
            "Named Entity Recognition:\n",
            "GPE: 1\n",
            "PERSON: 1\n",
            "ORDINAL: 1\n",
            "CARDINAL: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "import nltk\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Text to analyze\n",
        "text = df['clean_review'].iloc[0]\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Part-of-speech tagging and counting\n",
        "pos_counts = {'NOUN': 0, 'VERB': 0, 'ADJ': 0, 'ADV': 0}\n",
        "for token in doc:\n",
        "    if token.pos_ in pos_counts:\n",
        "        pos_counts[token.pos_] += 1\n",
        "\n",
        "# Print POS counts\n",
        "print(\"POS Counts:\")\n",
        "for pos, count in pos_counts.items():\n",
        "    print(f\"{pos}: {count}\")\n",
        "\n",
        "# Dependency Parsing\n",
        "print(\"\\nDependency Parsing Trees:\")\n",
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        print(token.text, \"-->\", token.dep_, \"-->\", token.head.text)\n",
        "\n",
        "# Named Entity Recognition (NER) and counting\n",
        "ner_counts = {}\n",
        "for ent in doc.ents:\n",
        "    ner_counts[ent.label_] = ner_counts.get(ent.label_, 0) + 1\n",
        "\n",
        "# Print NER counts\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "for ent_label, count in ner_counts.items():\n",
        "    print(f\"{ent_label}: {count}\")\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "Constituency parsing breaks a sentence into phrases and clauses based on grammar rules, showing their hierarchical structure in a tree. For example, in \"Barbi directs two films,\" \"Barbi\" is the subject, \"directs\" is the verb, and \"two films\" is the object.\n",
        "\n",
        "Dependency parsing focuses on word relationships, representing them as directed links. In the same sentence, \"directs\" is the root word, \"Barbi\" is the subject, and \"two films\" is the object, shown through arrows indicating dependencies.\n",
        "\n",
        "In essence, constituency parsing reveals the sentence's grammar-based hierarchy, while dependency parsing emphasizes word connections. Both provide valuable insights into sentence structure and relationships.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ceJ2JEzaHR2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''I tried working on another topic in the first question, but I was not able to gather more data from other sources. However, IMDB review gathering is a little easier compared to others, and clearing the text data is the easy part, but I face some issues in the 3rd question on Constituency Parsing and Dependency Parsing.'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1d3bd91e-4b49-44c6-8d30-861a820feffb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I tried working on another topic in the first question, but I was not able to gather more data from other sources. However, IMDB review gathering is a little easier compared to others, and clearing the text data is the easy part, but I face some issues in the 3rd question on Constituency Parsing and Dependency Parsing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}