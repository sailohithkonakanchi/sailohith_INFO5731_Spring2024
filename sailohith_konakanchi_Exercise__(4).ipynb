{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1a468a-6bcf-40f4-dd90-a8f8f9255afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['These Jordanâ€™s are authentic & very well made. All leather! My 10 year old '\n",
            " 'grandson loved them, he put them on immediately upon opening. They fit '\n",
            " 'perfectly & comfortably',\n",
            " 'My son loves these and only wears them for bball practice or a game. They '\n",
            " 'have held up great and still look new',\n",
            " 'They a great looking shoes comfortable, & are pretty wide I usually get wide '\n",
            " 'made shoes to fit right, but these are good in the size 13 fit.',\n",
            " 'The box was slightly smashed and thought shoes were damaged. No damage to '\n",
            " 'the shoes. Very comfortable and no major issues (at this point): hopefully '\n",
            " 'no issues for a long time.']\n",
            "[['these', 'jordan', 'are', 'authentic', 'very', 'well', 'made', 'all', 'leather', 'my', 'year', 'old', 'grandson', 'loved', 'them', 'he', 'put', 'them', 'on', 'immediately', 'upon', 'opening', 'they', 'fit', 'perfectly', 'comfortably'], ['my', 'son', 'loves', 'these', 'and', 'only', 'wears', 'them', 'for', 'bball', 'practice', 'or', 'game', 'they', 'have', 'held', 'up', 'great', 'and', 'still', 'look', 'new'], ['they', 'great', 'looking', 'shoes', 'comfortable', 'are', 'pretty', 'wide', 'usually', 'get', 'wide', 'made', 'shoes', 'to', 'fit', 'right', 'but', 'these', 'are', 'good', 'in', 'the', 'size', 'fit'], ['the', 'box', 'was', 'slightly', 'smashed', 'and', 'thought', 'shoes', 'were', 'damaged', 'no', 'damage', 'to', 'the', 'shoes', 'very', 'comfortable', 'and', 'no', 'major', 'issues', 'at', 'this', 'point', 'hopefully', 'no', 'issues', 'for', 'long', 'time']]\n",
            "['these', 'jordan', 'are', 'authentic', 'very', 'well', 'made', 'all', 'leather', 'my', 'year', 'old', 'grandson', 'loved', 'them', 'he', 'put', 'them', 'on', 'immediately', 'upon', 'opening', 'they', 'fit', 'perfectly', 'comfortably']\n",
            "[['well', 'make', 'leather', 'year', 'old', 'grandson', 'love', 'put', 'immediately', 'open', 'fit', 'perfectly', 'comfortably'], ['son', 'love', 'wear', 'bball', 'practice', 'game', 'hold', 'great', 'still', 'look', 'new'], ['great', 'look', 'shoe', 'comfortable', 'pretty', 'wide', 'usually', 'get', 'wide', 'make', 'shoe', 'fit', 'right', 'good', 'size', 'fit'], ['slightly', 'smash', 'think', 'shoe', 'damage', 'damage', 'shoe', 'comfortable', 'major', 'issue', 'point', 'hopefully', 'issue', 'long', 'time']]\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(5, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(1, 2), (6, 1), (15, 1), (17, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 2)], [(23, 1), (28, 2), (32, 2), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]]\n",
            "[(0,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (1,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (2,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (3,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (4,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (5,\n",
            "  '0.080*\"new\" + 0.080*\"bball\" + 0.080*\"son\" + 0.080*\"wear\" + 0.080*\"still\" + '\n",
            "  '0.080*\"love\" + 0.080*\"hold\" + 0.080*\"practice\" + 0.080*\"game\" + '\n",
            "  '0.080*\"great\"'),\n",
            " (6,\n",
            "  '0.122*\"shoe\" + 0.062*\"fit\" + 0.062*\"issue\" + 0.062*\"damage\" + 0.062*\"wide\" '\n",
            "  '+ 0.062*\"comfortable\" + 0.032*\"look\" + 0.032*\"great\" + 0.032*\"make\" + '\n",
            "  '0.032*\"pretty\"'),\n",
            " (7,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (8,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (9,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (10,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (11,\n",
            "  '0.070*\"old\" + 0.070*\"love\" + 0.070*\"put\" + 0.070*\"perfectly\" + 0.070*\"year\" '\n",
            "  '+ 0.070*\"comfortably\" + 0.070*\"grandson\" + 0.070*\"leather\" + 0.070*\"well\" + '\n",
            "  '0.070*\"immediately\"'),\n",
            " (12,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (13,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (14,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (15,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (16,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (17,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (18,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"'),\n",
            " (19,\n",
            "  '0.024*\"wide\" + 0.024*\"hopefully\" + 0.024*\"get\" + 0.024*\"good\" + '\n",
            "  '0.024*\"pretty\" + 0.024*\"right\" + 0.024*\"shoe\" + 0.024*\"size\" + '\n",
            "  '0.024*\"usually\" + 0.024*\"wear\"')]\n",
            "\n",
            "Perplexity:  -5.1021794600920245\n",
            "\n",
            "Coherence Score:  0.3176765978242252\n"
          ]
        }
      ],
      "source": [
        "!pip install pyLDAvis\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "\n",
        "new_data = [\n",
        "    \"These Jordanâ€™s are authentic & very well made. All leather! My 10 year old grandson loved them, he put them on immediately upon opening. They fit perfectly & comfortably\",\n",
        "    \"My son loves these and only wears them for bball practice or a game. They have held up great and still look new\",\n",
        "    \"They a great looking shoes comfortable, & are pretty wide I usually get wide made shoes to fit right, but these are good in the size 13 fit.\",\n",
        "    \"The box was slightly smashed and thought shoes were damaged. No damage to the shoes. Very comfortable and no major issues (at this point): hopefully no issues for a long time.\"\n",
        "]\n",
        "\n",
        "# Convert to list\n",
        "data = new_data\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "pprint(data)\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words)\n",
        "\n",
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])\n",
        "\n",
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized)\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus)\n",
        "\n",
        "id2word[0]\n",
        "\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "\n",
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a31a4e0-0753-4982-9117-dce9b8d02159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Documents: 4\n",
            "[(0, '0.495*\"shoe\" + 0.334*\"fit\" + 0.305*\"comfort\" + 0.277*\"wide\" + 0.218*\"issu\"'), (1, '0.354*\"damag\" + 0.354*\"issu\" + -0.288*\"fit\" + -0.200*\"made\" + 0.178*\"shoe\"')]\n",
            "[0.45854376854875567, 0.29251419963123154, 0.4408805529290713]\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data():\n",
        "\n",
        "    documents_list = [\n",
        "        \"These Jordanâ€™s are authentic & very well made. All leather! My 10 year old grandson loved them, he put them on immediately upon opening. They fit perfectly & comfortably\",\n",
        "        \"My son loves these and only wears them for bball practice or a game. They have held up great and still look new\",\n",
        "        \"They a great looking shoes comfortable, & are pretty wide I usually get wide made shoes to fit right, but these are good in the size 13 fit.\",\n",
        "        \"The box was slightly smashed and thought shoes were damaged. No damage to the shoes. Very comfortable and no major issues (at this point): hopefully no issues for a long time.\"\n",
        "    ]\n",
        "    titles = [text[0:min(len(text), 100)] for text in documents_list]\n",
        "    print(\"Total Number of Documents:\", len(documents_list))\n",
        "    return documents_list, titles\n",
        "\n",
        "# Load data\n",
        "documents_list, _ = load_data()\n",
        "\n",
        "def preprocess_data(doc_set):\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts\n",
        "\n",
        "# Preprocess the data\n",
        "doc_clean = preprocess_data(documents_list)\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    # Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # Return dictionary and Document Term Matrix\n",
        "    return dictionary, doc_term_matrix\n",
        "\n",
        "def create_gensim_lsa_model(doc_clean):\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=2, id2word=dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=2, num_words=5))\n",
        "    return lsamodel\n",
        "\n",
        "def compute_coherence_values(doc_clean, stop, start=2, step=1):\n",
        "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
        "\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Call the function to create the LSA model\n",
        "lsa_model = create_gensim_lsa_model(doc_clean)\n",
        "\n",
        "# Call the function to compute coherence values\n",
        "model_list, coherence_values = compute_coherence_values(doc_clean, stop=5)\n",
        "print(coherence_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7b8a91-44b4-4ecc-ba26-d82e7e23a8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Topics: 2  - Coherence Value: 0.6099060769195582\n",
            "Num Topics: 3  - Coherence Value: 0.8862701315112353\n",
            "Num Topics: 4  - Coherence Value: 0.893311249727419\n",
            "Num Topics: 5  - Coherence Value: 0.5718174074141513\n",
            "Num Topics: 6  - Coherence Value: 0.7643034153506361\n",
            "Num Topics: 7  - Coherence Value: 0.6144389832488342\n",
            "Num Topics: 8  - Coherence Value: 0.7358515356695429\n",
            "Num Topics: 9  - Coherence Value: 0.6153454761799311\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from gensim import corpora, models\n",
        "import numpy as np\n",
        "import pyLDAvis\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "try:\n",
        "    import seaborn\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "\n",
        "texts = [\n",
        "\"These Jordanâ€™s are authentic & very well made. All leather! My 10 year old grandson loved them, he put them on immediately upon opening. They fit perfectly & comfortably\",\n",
        "    \"My son loves these and only wears them for bball practice or a game. They have held up great and still look new\",\n",
        "    \"They a great looking shoes comfortable, & are pretty wide I usually get wide made shoes to fit right, but these are good in the size 13 fit.\",\n",
        "    \"The box was slightly smashed and thought shoes were damaged. No damage to the shoes. Very comfortable and no major issues (at this point): hopefully no issues for a long time.\"\n",
        "]\n",
        "\n",
        "# Tokenize and preprocess for the given text\n",
        "tokenized_text = [text.lower().split() for text in texts]\n",
        "\n",
        "# Creating the dictionary and corpus\n",
        "dictionary = corpora.Dictionary(tokenized_text)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "# Computing coherence values\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=tokenized_text, start=2, limit=10, step=1)\n",
        "\n",
        "# Print the coherence values\n",
        "for num_topics, coherence_val in zip(range(2, 10), coherence_values):\n",
        "    print(\"Num Topics:\", num_topics, \" - Coherence Value:\", coherence_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade bertopic\n",
        "!pip install bertopic\n",
        "# Importing the necessary library\n",
        "from bertopic import BERTopic\n",
        "\n",
        "\n",
        "data = [\n",
        "    \"These Jordanâ€™s are authentic & very well made.\",\n",
        "    \"All leather! My 10-year-old grandson loved them, he put them on immediately upon opening.\",\n",
        "    \"They fit perfectly & comfortably.\",\n",
        "    \"My son loves these and only wears them for basketball practice or a game.\",\n",
        "    \"They have held up great and still look new.\",\n",
        "    \"They are great-looking shoes, comfortable, & are pretty wide. I usually get wide shoes to fit right,\",\n",
        "    \"but these are good in the size 13 fit.\",\n",
        "    \"The box was slightly smashed and thought the shoes were damaged.\",\n",
        "    \"No damage to the shoes. Very comfortable and no major issues at this point,\",\n",
        "    \"hopefully no issues for a long time.\"\n",
        "]\n",
        "\n",
        "# Initialize BERTopic\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Fit BERTopic on the provided data\n",
        "topics, probs = topic_model.fit_transform(data)\n",
        "\n",
        "# Get frequency of topics\n",
        "freq = topic_model.get_topic_info()\n",
        "print(freq.head(5))\n",
        "\n",
        "# Get top words associated with a specific topic\n",
        "topic_words = topic_model.get_topic(0)\n",
        "print(topic_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5a6722d8c2d345ad9e8ebc067a6eb80f",
            "bebf393f9e184fb4b251f77fad829835",
            "7860d5c9bcf74148ab56e893d0fa6db9",
            "85ff15dfd2484ad39d1c63ad4570f069",
            "d1864d1767c649638cd1045863be0f39",
            "af6ae2f5a09e44969db329b776bc15fe",
            "acaaead9006046e1bebab137ab7caf16",
            "cb08393715ec41b5b8f542d1f6688350",
            "8ab44635bba14be7bcf4e5e4efda2a65",
            "845935c4940b4685bce15821b9df9c27",
            "91a5a2525f5f45549dbe992b8fb34a7a"
          ]
        },
        "id": "XxosHJeRAenK",
        "outputId": "2dcb02ea-edf6-4aa6-dfe1-fa60266f8f12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.6.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.6.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 03:04:31,427 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a6722d8c2d345ad9e8ebc067a6eb80f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 03:04:32,489 - BERTopic - Embedding - Completed âœ“\n",
            "2024-03-30 03:04:32,491 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-30 03:04:36,748 - BERTopic - Dimensionality - Completed âœ“\n",
            "2024-03-30 03:04:36,750 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-30 03:04:36,760 - BERTopic - Cluster - Completed âœ“\n",
            "2024-03-30 03:04:36,768 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-30 03:04:36,787 - BERTopic - Representation - Completed âœ“\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Topic  Count                  Name  \\\n",
            "0     -1     10  -1_the_shoes_and_are   \n",
            "\n",
            "                                      Representation  \\\n",
            "0  [the, shoes, and, are, them, these, they, fit,...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [My son loves these and only wears them for ba...  \n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''''Based on the data, the four topic modeling techniques LDA, NMF, LSA, and BERTopicâ€”each have advantages. LDA provides intelligible topics but may omit intricate relationships. While NMF and LSA uncover hidden patterns, they may be difficult to understand. With the help of BERT, BERTopic can better comprehend context and delineate topics, which makes it suitable for large and diverse datasets. Therefore, because BERTopic understands words in context and generates clear topics, it's probably the best option if you need topics that make sense and demonstrate connections.\n",
        "'''''"
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c10efd22-02ac-4ed7-ba68-ed8bedf28cfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"''Based on the data, the four topic modeling techniques LDA, NMF, LSA, and BERTopicâ€”each have advantages. LDA provides intelligible topics but may omit intricate relationships. While NMF and LSA uncover hidden patterns, they may be difficult to understand. With the help of BERT, BERTopic can better comprehend context and delineate topics, which makes it suitable for large and diverse datasets. Therefore, because BERTopic understands words in context and generates clear topics, it's probably the best option if you need topics that make sense and demonstrate connections.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        " working with text data and extracting features using various topic modeling algorithms provided a valuable learning experience. I learned algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA),BERT model . It was a little difficult to determine the ideal number of topics (K) for each algorithm based on coherence scores because it needed some trial and error and an understanding of the trade-offs between interpretability and model complexity. By generating topics using LDA, LSA, lda2vec, and BERTopic, we gain insights into the underlying structures and themes present in the text data. Understanding and summarizing these topics provide valuable information for tasks such as document clustering, categorization, and summarization, which are essential in various NLP applications such as sentiment analysis, document recommendation, and information retrieval.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "da57dbd6-b0bd-4082-9416-c969fe6589aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n working with text data and extracting features using various topic modeling algorithms provided a valuable learning experience. I learned algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA),BERT model . It was a little difficult to determine the ideal number of topics (K) for each algorithm based on coherence scores because it needed some trial and error and an understanding of the trade-offs between interpretability and model complexity. By generating topics using LDA, LSA, lda2vec, and BERTopic, we gain insights into the underlying structures and themes present in the text data. Understanding and summarizing these topics provide valuable information for tasks such as document clustering, categorization, and summarization, which are essential in various NLP applications such as sentiment analysis, document recommendation, and information retrieval.\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a6722d8c2d345ad9e8ebc067a6eb80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bebf393f9e184fb4b251f77fad829835",
              "IPY_MODEL_7860d5c9bcf74148ab56e893d0fa6db9",
              "IPY_MODEL_85ff15dfd2484ad39d1c63ad4570f069"
            ],
            "layout": "IPY_MODEL_d1864d1767c649638cd1045863be0f39"
          }
        },
        "bebf393f9e184fb4b251f77fad829835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6ae2f5a09e44969db329b776bc15fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_acaaead9006046e1bebab137ab7caf16",
            "value": "Batches:â€‡100%"
          }
        },
        "7860d5c9bcf74148ab56e893d0fa6db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb08393715ec41b5b8f542d1f6688350",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ab44635bba14be7bcf4e5e4efda2a65",
            "value": 1
          }
        },
        "85ff15dfd2484ad39d1c63ad4570f069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_845935c4940b4685bce15821b9df9c27",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_91a5a2525f5f45549dbe992b8fb34a7a",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡4.74it/s]"
          }
        },
        "d1864d1767c649638cd1045863be0f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6ae2f5a09e44969db329b776bc15fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaaead9006046e1bebab137ab7caf16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb08393715ec41b5b8f542d1f6688350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab44635bba14be7bcf4e5e4efda2a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "845935c4940b4685bce15821b9df9c27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a5a2525f5f45549dbe992b8fb34a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}